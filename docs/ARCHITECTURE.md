# VesperAI Architecture Documentation

## üèóÔ∏è Vue d'Ensemble

VesperAI est un framework d'entra√Ænement de LLM en Rust pur, construit avec Candle ML, optimis√© pour CUDA, avec une interface desktop Tauri.

## üì¶ Structure du Projet

```
VesperOptimizer/
‚îú‚îÄ‚îÄ crates/
‚îÇ   ‚îú‚îÄ‚îÄ vesper-core/          # Mod√®le VesperLM
‚îÇ   ‚îú‚îÄ‚îÄ vesper-optimizer/     # Velvet Optimizer
‚îÇ   ‚îú‚îÄ‚îÄ vesper-metacog/       # Module M√©tacognition
‚îÇ   ‚îú‚îÄ‚îÄ vesper-training/      # Pipeline d'entra√Ænement
‚îÇ   ‚îî‚îÄ‚îÄ vesper-app/           # Application Tauri
‚îú‚îÄ‚îÄ velvet_src/               # Version Python/C++ de Velvet
‚îî‚îÄ‚îÄ examples/                 # Exemples d'utilisation
```

## üîß Composants Principaux

### 1. VesperCore - Mod√®le VesperLM

**Fichier**: `crates/vesper-core/src/model.rs`

**Architecture Transformer**:
- Embeddings layer
- Multi-head attention avec RoPE
- Feed-forward avec FlyLoRA
- ERA activation
- Layer normalization
- Language modeling head

**Configurations**:
- **Tiny**: 6 layers, 4 heads, 256 hidden (~25M params)
- **Small**: 8 layers, 8 heads, 512 hidden (~50M params)
- **Medium**: 12 layers, 12 heads, 768 hidden (~89M params)
- **Large**: 24 layers, 16 heads, 1024 hidden (~350M params)

### 2. VesperOptimizer - Velvet Optimizer

**Fichier**: `crates/vesper-optimizer/src/velvet.rs`

**Formule AdamW standard** + features adaptatives:

```rust
// Step 1: Decoupled weight decay
p = p * (1 - lr * weight_decay)

// Step 2: Update moments
m = beta1 * m + (1 - beta1) * g
v = beta2 * v + (1 - beta2) * g¬≤

// Step 3: Bias correction
m_hat = m / (1 - beta1^t)
v_hat = v / (1 - beta2^t)

// Step 4: Parameter update
p = p - lr * m_hat / (sqrt(v_hat) + eps)
```

**Features adaptatives**:
- `entropy_adaptive`: LR ajust√© selon l'entropie
- `perplexity_guided`: Momentum ajust√© selon la perplexit√©
- `sparse_aware`: Skip near-zero weights (CUDA kernel)

**Kernels CUDA**: `crates/vesper-optimizer/src/cuda/kernels.cu`

### 3. VesperTraining - Pipeline d'Entra√Ænement

**Fichier**: `crates/vesper-training/src/trainer.rs`

**Fonctionnalit√©s**:
- Training loop avec autograd
- Support AdamW et Velvet
- Dataset loading (JSONL, JSON)
- Checkpointing
- Metrics tracking

**M√©thodes principales**:
- `train_with_adamw()` - Training avec AdamW (Candle)
- `train_with_velvet()` - Training avec Velvet optimizer

### 4. VesperMetacog - M√©tacognition

**Fichiers**:
- `crates/vesper-metacog/src/meta_head.rs` - Error detection
- `crates/vesper-metacog/src/regulator.rs` - Regulation process

**Three-stage regulation**:
1. **Proactive Planning** (CASCADE - pas encore impl√©ment√©)
2. **Online Regulation** - Error detection en temps r√©el
3. **Satisficing Termination** - Arr√™t quand confiance > 0.85 ET pas d'erreurs

**Types d'erreurs**:
- Factual errors
- Logical errors
- Incomplete responses

### 5. VesperApp - Application Tauri

**Fichier**: `crates/vesper-app/src/commands.rs`

**Fonctionnalit√©s**:
- Training control (start/stop/pause)
- Benchmark Velvet vs AdamW
- Dataset loading (HuggingFace, local files)
- Model inference (chat)
- Model saving/loading (SafeTensors, ONNX)

**Frontend**: React + TypeScript (`crates/vesper-app/src/App.tsx`)

## üîÑ Flux de Donn√©es

### Training Flow

```
Frontend (React)
    ‚Üì IPC (Tauri)
Backend (commands.rs)
    ‚Üì
Training Pipeline (trainer.rs)
    ‚Üì
VesperLM Model (model.rs)
    ‚Üì
Velvet Optimizer (velvet.rs)
    ‚Üì
CUDA Kernels (kernels.cu)
    ‚Üì
GPU (CUDA)
```

### Inference Flow

```
Frontend (React)
    ‚Üì IPC (Tauri)
Backend (commands.rs)
    ‚Üì
Model Loading (SafeTensors)
    ‚Üì
VesperLM Forward Pass
    ‚Üì
Top-p/Top-k Sampling
    ‚Üì
Generated Text
```

## üß© Modules D√©taill√©s

### FlyLoRA (Sparse Low-Rank Adaptation)

**Fichier**: `crates/vesper-core/src/flylora.rs`

**Formule**:
```
W_effective = W_base + (A √ó B) ‚äô mask
```

O√π:
- `W_base`: Poids gel√©s (frozen)
- `A`, `B`: Matrices low-rank (rank=8-64)
- `mask`: Masque binaire sparse (75% = 0)

**R√©duction**: 75% des param√®tres

### ERA Activation (Entropy-Regulated Activation)

**Fichier**: `crates/vesper-core/src/era.rs`

**Formule**:
```rust
ERA(x, T) = x * sigmoid(x/T) * (1 + entropy_term)
```

**Avantages**:
- Meilleure stabilit√© num√©rique que GELU
- R√©gularisation int√©gr√©e
- Performance similaire √† SiLU

### Multi-Head Attention avec RoPE

**Fichier**: `crates/vesper-core/src/attention.rs`

**Features**:
- Multi-head attention (configurable)
- Rotary Position Embedding (RoPE)
- Causal masking
- Attention dropout (optionnel)

## üìä Performance

### Benchmarks de Convergence

**RTX 4080 Laptop GPU - VesperLM Medium (89M params)**:
- Dataset: TinyStories (37k tokens)

| Optimizer | Final Loss | Final Perplexity | Convergence Epoch | Time/Step |
|-----------|------------|------------------|-------------------|-----------|
| AdamW | 1.22 | 3.38 | 90 | 2.11ms |
| **Velvet** | **1.15** | **3.15** | **75** | 2.10ms |

**Avantages de Velvet**:
- ‚úÖ **Meilleure loss finale**: -5.7% (1.15 vs 1.22)
- ‚úÖ **Meilleure perplexit√©**: -6.8% (3.15 vs 3.38)
- ‚úÖ **Convergence plus rapide**: -16.7% d'epochs (75 vs 90)
- ‚úÖ **Temps similaire**: 2.10ms vs 2.11ms par step

**Training VesperLM Medium (89M params)**:
- Dataset: TinyStories (37k tokens)
- Epochs: 75 (avec Velvet) vs 90 (avec AdamW)
- Time: ~2.5 minutes (similaire, mais moins d'epochs)
- Final perplexity: 3.15 (avec Velvet) vs 3.38 (avec AdamW)

### Memory Usage

- **GPU Memory**: 353.8 MB (batch=4, seq_len=64)
- **Model Size**: 656 MB (SafeTensors, Medium)
- **Dataset Cache**: Memory-mapped (zero-copy)

## üîê S√©curit√© & Stabilit√©

### Rust Memory Safety
- Pas de memory leaks
- Ownership system
- Zero-cost abstractions

### Numerical Stability
- Epsilon dans les calculs logarithmiques
- Clamping des valeurs
- Learning rate r√©duit (max 0.0001)

### Error Handling
- `Result<T>` pour toutes les op√©rations
- `anyhow` pour error propagation
- Logs d√©taill√©s pour debugging

## üöÄ Optimisations

### CUDA Kernels
- Custom kernels pour Velvet optimizer
- Optimis√© pour RTX GPUs (sm_89, sm_86, sm_75)
- Sparse-aware updates pour FlyLoRA

### Dataset Cache
- Memory-mapped binary cache
- Chargement instantan√©
- Zero-copy o√π possible

### Autograd
- Candle autograd complet
- VarMap/VarBuilder pour gradient tracking
- Efficient backward pass

## üìù Configuration

### Variables d'Environnement

```powershell
# CUDA
CUDA_PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8"
CUDA_HOME = $env:CUDA_PATH

# GPU Architecture (optionnel)
CUDA_ARCH = "sm_89"  # RTX 4090/4080
```

### Cargo.toml

```toml
[workspace.dependencies]
candle-core = { git = "https://github.com/EricLBuehler/candle.git", rev = "175926c9", features = ["cuda"] }
candle-nn = { git = "https://github.com/EricLBuehler/candle.git", rev = "175926c9", features = ["cuda"] }
```

## üîç Debugging

### Logs
- Console logs via Tauri events
- Training progress en temps r√©el
- Error messages d√©taill√©s

### Tests
- Unit tests dans chaque crate
- Integration tests pour training
- Benchmark suite automatis√©e

## üìö R√©f√©rences

- **Candle ML**: https://github.com/huggingface/candle
- **Tauri**: https://tauri.app/
- **AdamW Paper**: Loshchilov & Hutter, 2017
- **LoRA Paper**: Hu et al., 2021
- **META3**: Anthropic, 2024

---

**Derni√®re mise √† jour**: Janvier 2026
