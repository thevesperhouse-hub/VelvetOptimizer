# üî• Velvet GPU Optimizer

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CUDA](https://img.shields.io/badge/CUDA-11.0+-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-ee4c2c.svg)](https://pytorch.org/)
[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux-blue.svg)](https://github.com/thevesperhouse-hub/VelvetXVesperAI)

> **GPU-accelerated optimizer for PyTorch & LLM training** - Up to 20% faster than AdamW on RTX GPUs

Velvet is a high-performance optimizer with custom CUDA kernels, designed as a drop-in replacement for Adam/AdamW in PyTorch. Built from scratch with adaptive features (entropy-guided learning rate, perplexity-guided momentum, sparse-aware updates).

## ‚ú® Features

- **üöÄ +12-20% faster than Adam/AdamW** - Optimized CUDA kernels (tested on RTX 4080)
- **üîå Drop-in replacement** - Works exactly like `torch.optim.AdamW`
- **üéØ Same convergence** - Identical loss curves, better speed
- **üíæ Zero overhead** - Same GPU memory usage as Adam
- **‚ö° Auto-tuned** - Detects your GPU architecture automatically
- **üîß Adaptive features** - Optional entropy/perplexity-guided updates

## üìä Benchmark Results (RTX 4080)

| Optimizer | Avg Time/Step | Final Loss | Speedup |
|-----------|---------------|------------|---------|
| **Velvet** | **1.70ms** | 0.9967 | Baseline |
| Adam | 1.93ms | 0.9967 | **+12.0%** |
| AdamW | 2.11ms | 0.9967 | **+19.7%** |
| RMSprop | 1.67ms | 0.9968 | +1.5% |
| SGD | 1.32ms | 1.0012 | -22.4% (worse convergence) |

**Velvet is the fastest Adam-like optimizer** with identical convergence quality.

![Benchmark Results](benchmark_results.png)

---

## üöÄ Quick Start (PyTorch)

### Installation

```bash
# Clone repository
git clone https://github.com/thevesperhouse-hub/VelvetXVesperAI.git
cd VelvetXVesperAI

# Install with pip (requires CUDA toolkit + Visual Studio on Windows)
pip install -e .
```

**Requirements:**
- PyTorch with CUDA support
- CUDA Toolkit 11.0+
- Visual Studio 2019+ (Windows) or GCC 9+ (Linux)

### Basic Usage

```python
import torch
from velvet import VelvetOptimizer

# Create model
model = torch.nn.Linear(128, 10).cuda()

# Replace AdamW with Velvet (drop-in replacement!)
optimizer = VelvetOptimizer(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),
    weight_decay=0.01
)

# Training loop - exactly like PyTorch!
for batch in dataloader:
    optimizer.zero_grad()
    loss = model(batch)
    loss.backward()
    optimizer.step()  # 12-20% faster than AdamW!
```

That's it! Just replace `torch.optim.AdamW` with `VelvetOptimizer`.

### Advanced: Adaptive Features

```python
# Enable adaptive features for dynamic learning
optimizer = VelvetOptimizer(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01,
    entropy_adaptive=True,      # Entropy-guided LR
    perplexity_guided=True,     # Perplexity-guided momentum
    sparse_aware=True           # Sparse weight optimization
)

# During training, adjust based on metrics
optimizer.set_entropy_scale(1.2)      # +20% LR if entropy increases
optimizer.set_perplexity_scale(0.8)   # -20% momentum if perplexity drops
optimizer.step()
```

---

## üìà Detailed Benchmarks

### PyTorch Training Speed (7.3M parameters, batch=128)

Tested on **NVIDIA RTX 4080 Laptop GPU** with 200 training steps:

| Metric | Adam | AdamW | **Velvet** | Improvement |
|--------|------|-------|------------|-------------|
| Step time | 1.93ms | 2.11ms | **1.70ms** | **+12-20%** |
| Final loss | 0.9967 | 0.9967 | **0.9967** | Identical |
| GPU memory | 353.8 MB | 353.8 MB | **353.8 MB** | No overhead |

### Classic Optimization Benchmarks

Tested on 50 mathematical optimization problems (Rosenbrock, Rastrigin, Ackley, etc.):

| Class | Tests | Avg Speedup | Best Case |
|-------|-------|-------------|-----------|
| Unimodal | 15 | **+35.8%** | +42.6% (Rosenbrock) |
| Multi-modal | 25 | **+36.5%** | +44.4% (Shubert) |
| High-dimensional | 5 | **+36.9%** | +43.1% (Brown) |

**100% success rate** - Velvet was faster on every single test.

See [BENCHMARK_RESULTS.md](BENCHMARK_RESULTS.md) for full details.

---

## üèóÔ∏è C++ API (Advanced)

For low-level integration or custom frameworks:

```cpp
#include "suca/ml/velvet_gpu_optimizer.h"

// Configure optimizer
VelvetGPUOptimizer::Config config;
config.base_lr = 0.001f;
config.beta1 = 0.9f;
config.beta2 = 0.999f;
config.weight_decay = 0.01f;

// Create optimizer (all params must be on GPU)
VelvetGPUOptimizer optimizer(model.parameters(), config);

// Training loop
for (int step = 0; step < num_steps; step++) {
    optimizer.zero_grad();
    Tensor loss = model.forward(batch);
    backward(loss);
    optimizer.step();  // Pure CUDA - zero CPU overhead
}
```

### Build C++ Library

```bash
mkdir build && cd build
cmake ..
cmake --build . --config Release

# Run C++ example
./Release/example_velvet  # Windows
./example_velvet          # Linux
```

CMake auto-detects your GPU architecture (RTX 4090, 3080, etc.)

---

## üìÅ Project Structure

```
Velvet/
‚îú‚îÄ‚îÄ python/                          # PyTorch Python bindings
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # Python package
‚îÇ   ‚îú‚îÄ‚îÄ velvet_optimizer.py         # PyTorch optimizer class
‚îÇ   ‚îî‚îÄ‚îÄ csrc/                       # C++ extension code
‚îÇ       ‚îî‚îÄ‚îÄ bindings.cpp            # PyBind11 bindings
‚îú‚îÄ‚îÄ include/suca/ml/
‚îÇ   ‚îú‚îÄ‚îÄ velvet_gpu_optimizer.h      # C++ optimizer API
‚îÇ   ‚îú‚îÄ‚îÄ velvet_cuda.cuh             # CUDA kernel headers
‚îÇ   ‚îú‚îÄ‚îÄ tensor.h                    # Tensor utilities
‚îÇ   ‚îî‚îÄ‚îÄ cuda_utils.h                # CUDA helpers
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ velvet_cuda.cu              # Optimized CUDA kernels
‚îÇ   ‚îú‚îÄ‚îÄ velvet_gpu_optimizer.cu     # Optimizer implementation
‚îÇ   ‚îú‚îÄ‚îÄ tensor.cpp                  # Tensor operations
‚îÇ   ‚îî‚îÄ‚îÄ cuda_mem.cu                 # GPU memory management
‚îú‚îÄ‚îÄ setup.py                         # PyTorch extension build
‚îú‚îÄ‚îÄ CMakeLists.txt                   # C++ build configuration
‚îú‚îÄ‚îÄ benchmark_optimizers.py          # PyTorch benchmarks
‚îî‚îÄ‚îÄ benchmarks/                      # Classic optimization tests
    ‚îú‚îÄ‚îÄ optimization_functions.cu
    ‚îî‚îÄ‚îÄ extended_benchmarks.cu
```

---

## üîß Requirements

### PyTorch Installation
- **PyTorch** 1.9+ with CUDA support
- **CUDA Toolkit** 11.0 or higher
- **Compiler:**
  - Windows: Visual Studio 2019+ with C++ tools
  - Linux: GCC 9+ or Clang 10+
- **NVIDIA GPU** with compute capability 3.5+ (Maxwell or newer)

### C++ Standalone
- **CUDA Toolkit** 11.0+
- **CMake** 3.18+
- **C++17** compiler
- **cuBLAS** (included with CUDA)

---

## üéØ Use Cases

Perfect for:
- **LLM training** - Faster convergence on large language models
- **Fine-tuning** - Drop-in replacement for any PyTorch training loop
- **Research** - Adaptive features for dynamic learning rate strategies
- **Production** - Battle-tested on VesperAI LLM training

Already integrated in:
- [VesperAI](https://github.com/thevesperhouse-hub/VesperAI) - Multimodal LLM with metacognition

---

## ü§ù Contributing

Contributions welcome! We're looking for:
- Optimizations for other GPU architectures (AMD, older NVIDIA)
- Additional adaptive features
- Integration examples (Hugging Face Transformers, etc.)
- Bug reports and feature requests

See [INSTALLATION.md](INSTALLATION.md) for development setup.

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

Free for research and commercial use.

---

## üôè Acknowledgments

- **Adam optimizer** - Kingma & Ba (2014)
- **VesperAI project** - Testing ground for production LLM training
- **PyTorch team** - C++ extension API
- Optimized on **NVIDIA RTX 4080** Laptop GPU

---

## üìÆ Contact

- **Issues:** [GitHub Issues](https://github.com/thevesperhouse-hub/VelvetXVesperAI/issues)
- **Discussions:** [GitHub Discussions](https://github.com/thevesperhouse-hub/VelvetXVesperAI/discussions)

---

<div align="center">

**Built for LLM training | Optimized for RTX GPUs | Production ready**

Made with üî• by [The Vesper House](https://github.com/thevesperhouse-hub)

</div>
