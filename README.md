# ğŸ”¥ VesperAI - Rust Edition

[![License: Proprietary](https://img.shields.io/badge/License-Proprietary-red.svg)](LICENSE)
[![Rust](https://img.shields.io/badge/Rust-1.83+-orange.svg)](https://www.rust-lang.org/)
[![CUDA](https://img.shields.io/badge/CUDA-12.8-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![Candle](https://img.shields.io/badge/Candle-0.9.1-blue.svg)](https://github.com/huggingface/candle)
[![Tauri](https://img.shields.io/badge/Tauri-2.0-purple.svg)](https://tauri.app/)

> Framework d'entraÃ®nement LLM haute performance en Rust pur avec Candle ML

---

## ğŸ¯ Ã‰tat du Projet

### âœ… Fonctionnel
- **VesperLM** - ModÃ¨le transformer complet (Small/Medium/Large)
- **Training CUDA** - EntraÃ®nement GPU avec autograd
- **Velvet Optimizer** - AdamW amÃ©liorÃ© avec LR adaptatif
- **FlyLoRA** - Sparse Low-Rank Adaptation (75% rÃ©duction params)
- **ERA Activation** - Entropy-Regularized Activation
- **CamemBERT Tokenizer** - Tokenization franÃ§ais
- **Dataset Cache** - Memory-mapped binary cache (chargement instantanÃ©)
- **SafeTensors** - Sauvegarde/chargement modÃ¨les
- **Chat Inference** - GÃ©nÃ©ration de texte avec top-p/top-k sampling
- **Application Tauri** - Interface desktop complÃ¨te

### ğŸ”§ ProblÃ¨mes RÃ©solus
- **MSVC Linker** - Configuration correcte des toolchains Windows
- **CUDA 12.8** - CompatibilitÃ© via fork EricLBuehler/candle
- **bindgen_cuda** - Fix via fork guoqingbao/bindgen_cuda
- **Tensor Layout** - Corrections `.contiguous()` aprÃ¨s transpose
- **NaN Loss** - StabilitÃ© numÃ©rique (epsilon, clamping, LR bas)
- **Shape Mismatches** - FlyLoRA et ERA corrigÃ©s

### â³ Reste Ã  Faire
- [ ] **Dataset franÃ§ais** - TÃ©lÃ©charger Claire-Dialogue-French (gated, nÃ©cessite auth HuggingFace)
- [ ] **GÃ©nÃ©ralisation** - EntraÃ®ner sur plus de donnÃ©es pour Ã©viter l'overfitting
- [ ] **Multi-GPU** - Support NCCL pour entraÃ®nement distribuÃ©
- [ ] **Quantization** - INT8/INT4 pour infÃ©rence plus rapide
- [ ] **Streaming** - GÃ©nÃ©ration token par token dans le chat

---

## ğŸ“‹ Table des MatiÃ¨res

- [Ã‰tat du Projet](#-Ã©tat-du-projet)
- [Stack Technique ComplÃ¨te](#-stack-technique-complÃ¨te)
- [PrÃ©requis SystÃ¨me](#-prÃ©requis-systÃ¨me)
- [Installation DÃ©taillÃ©e](#-installation-dÃ©taillÃ©e)
- [Architecture](#-architecture)
- [Composants](#-composants)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ›  Stack Technique ComplÃ¨te

### Backend (Rust)

| Composant | Version | RÃ´le |
|-----------|---------|------|
| **Rust** | 1.83+ | Langage principal, memory-safe, zero-cost abstractions |
| **Candle** | 0.9.1 (EricLBuehler fork) | Framework ML Rust, tenseurs GPU/CPU |
| **cudarc** | 0.10 | Bindings CUDA low-level pour Rust |
| **Tokio** | 1.x | Runtime async pour I/O non-bloquant |
| **Serde** | 1.x | SÃ©rialisation JSON/binaire |
| **Tauri** | 2.0 | Framework desktop app (Rust backend) |

### Frontend (Web/Desktop)

| Composant | Version | RÃ´le |
|-----------|---------|------|
| **React** | 18.2 | UI components |
| **TypeScript** | 5.3 | Type safety frontend |
| **Vite** | 5.0 | Build tool ultra-rapide |
| **TailwindCSS** | 3.3 | Styling utility-first |
| **Lucide React** | 0.300 | IcÃ´nes |
| **OGL** | 1.0 | WebGL pour effets Aurora |

### GPU/CUDA

| Composant | Version | RÃ´le |
|-----------|---------|------|
| **CUDA Toolkit** | 12.8 | Runtime et compilateur nvcc |
| **cuDNN** | 9.x | Optimisations deep learning |
| **NCCL** | 2.x | Multi-GPU communication (optionnel) |

### Build Tools (Windows)

| Composant | Version | RÃ´le |
|-----------|---------|------|
| **Visual Studio Build Tools** | 2022 (17.x) | Compilateur MSVC |
| **MSVC** | v143 | Toolchain C++ |
| **Windows SDK** | 10.0.22621+ | Headers systÃ¨me |
| **CMake** | 3.28+ | Build system (pour dÃ©pendances) |

---

## ğŸ’» PrÃ©requis SystÃ¨me

### Windows 11/10

#### 1. Visual Studio Build Tools 2022

**TÃ©lÃ©chargement**: https://visualstudio.microsoft.com/visual-cpp-build-tools/

**Composants requis** (cocher lors de l'installation):
```
â˜‘ Desktop development with C++
  â”œâ”€â”€ MSVC v143 - VS 2022 C++ x64/x86 build tools (Latest)
  â”œâ”€â”€ Windows 11 SDK (10.0.22621.0) ou Windows 10 SDK
  â”œâ”€â”€ C++ CMake tools for Windows
  â””â”€â”€ C++ ATL for latest v143 build tools (x86 & x64)
```

**Variables d'environnement** (automatiques aprÃ¨s install):
```powershell
# VÃ©rifier que ces paths existent
$env:VCToolsInstallDir  # ex: C:\Program Files\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.38.33130\
```

#### 2. CUDA Toolkit 12.8

**TÃ©lÃ©chargement**: https://developer.nvidia.com/cuda-12-8-0-download-archive

**Installation**:
```powershell
# Chemin par dÃ©faut
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\

# VÃ©rifier l'installation
nvcc --version
# nvcc: NVIDIA (R) Cuda compiler driver
# Cuda compilation tools, release 12.8, V12.8.xxx
```

**Variables d'environnement requises**:
```powershell
# Dans les variables systÃ¨me
CUDA_PATH = C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8
CUDA_HOME = C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8

# Ajouter au PATH
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\libnvvp
```

#### 3. Rust Toolchain

```powershell
# Installer rustup (si pas dÃ©jÃ  fait)
winget install Rustlang.Rustup

# Installer la toolchain stable
rustup default stable

# VÃ©rifier
rustc --version
# rustc 1.83.0 (90b35a623 2024-11-26)

cargo --version
# cargo 1.83.0 (5ffbef321 2024-10-29)
```

#### 4. Node.js (pour le frontend Tauri)

```powershell
# Via winget
winget install OpenJS.NodeJS.LTS

# VÃ©rifier
node --version
# v20.x.x

npm --version
# 10.x.x
```

---

## ğŸ“¥ Installation DÃ©taillÃ©e

### Ã‰tape 1: Cloner le repo

```powershell
git clone https://github.com/thevesperhouse-hub/VesperAI.git
cd VesperAI
```

### Ã‰tape 2: VÃ©rifier les prÃ©requis

```powershell
# Script de vÃ©rification
.\scripts\check-prereqs.ps1

# Ou manuellement:
rustc --version          # >= 1.83
nvcc --version           # CUDA 12.8
cl.exe                   # MSVC disponible (ouvrir "x64 Native Tools Command Prompt")
node --version           # >= 20
```

### Ã‰tape 3: Build backend Rust

```powershell
# Build debug (plus rapide, pour dev)
cargo build

# Build release (optimisÃ©, pour prod)
cargo build --release

# Temps de build estimÃ©:
# - Debug: ~3-5 min (premiÃ¨re fois)
# - Release: ~8-15 min (premiÃ¨re fois)
# - IncrÃ©mental: ~10-30s
```

### Ã‰tape 4: Setup frontend

```powershell
cd crates/vesper-app
npm install
```

### Ã‰tape 5: Lancer l'application

```powershell
# Mode dÃ©veloppement (hot-reload)
npm run tauri dev

# Build production
npm run tauri build
```

---

## ğŸ— Architecture

```
VesperOptimizer/
â”œâ”€â”€ Cargo.toml                 # Workspace config
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ vesper-core/           # ModÃ¨le VesperLM
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ model.rs       # Architecture transformer
â”‚   â”‚   â”‚   â”œâ”€â”€ attention.rs   # Multi-head attention + RoPE
â”‚   â”‚   â”‚   â”œâ”€â”€ flylora.rs     # Sparse LoRA (75% param reduction)
â”‚   â”‚   â”‚   â””â”€â”€ era.rs         # Entropy-Regulated Activation
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â”‚
â”‚   â”œâ”€â”€ vesper-optimizer/      # Optimiseur Velvet
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ velvet.rs      # AdamW amÃ©liorÃ© avec features adaptatives
â”‚   â”‚   â”‚   â””â”€â”€ cuda/          # Kernels CUDA custom
â”‚   â”‚   â”‚       â”œâ”€â”€ mod.rs     # Wrapper Rust
â”‚   â”‚   â”‚       â””â”€â”€ kernels.cu # Code CUDA C++
â”‚   â”‚   â”œâ”€â”€ build.rs           # Script compilation CUDA
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â”‚
â”‚   â”œâ”€â”€ vesper-metacog/        # Module mÃ©tacognition
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ meta_head.rs   # TÃªte de prÃ©diction d'erreur
â”‚   â”‚       â””â”€â”€ regulator.rs   # RÃ©gulateur adaptatif
â”‚   â”‚
â”‚   â”œâ”€â”€ vesper-training/       # Pipeline d'entraÃ®nement
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ trainer.rs     # Boucle d'entraÃ®nement
â”‚   â”‚       â”œâ”€â”€ dataset.rs     # Chargement JSONL/JSON
â”‚   â”‚       â””â”€â”€ auto_scale.rs  # Chinchilla scaling laws
â”‚   â”‚
â”‚   â””â”€â”€ vesper-app/            # Application Tauri
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ main.rs        # Entry point Tauri
â”‚       â”‚   â”œâ”€â”€ commands.rs    # API Rust <-> Frontend
â”‚       â”‚   â”œâ”€â”€ App.tsx        # UI React principale
â”‚       â”‚   â””â”€â”€ components/    # Composants React
â”‚       â”œâ”€â”€ package.json       # DÃ©pendances npm
â”‚       â”œâ”€â”€ tauri.conf.json    # Config Tauri
â”‚       â””â”€â”€ tailwind.config.js # Config TailwindCSS
â”‚
â””â”€â”€ target/                    # Build output
    â”œâ”€â”€ debug/
    â””â”€â”€ release/
```

---

## ğŸ§© Composants

### 1. Candle ML Framework

**Pourquoi Candle plutÃ´t que PyTorch?**
- **Performance**: Pas d'overhead Python, tenseurs natifs Rust
- **Memory safety**: Pas de memory leaks, ownership system
- **Compilation**: AOT compilation, pas de JIT overhead
- **CUDA**: Support natif via cudarc

**Fork utilisÃ©**: `EricLBuehler/candle` (rev 175926c9)
- Fixes pour CUDA 12.8
- Meilleur support Windows
- Optimisations pour mistral.rs

```toml
# Cargo.toml
candle-core = { git = "https://github.com/EricLBuehler/candle.git", rev = "175926c9", features = ["cuda"] }
candle-nn = { git = "https://github.com/EricLBuehler/candle.git", rev = "175926c9", features = ["cuda"] }
```

**Patch critique pour bindgen_cuda** (rÃ©sout erreurs de linking CUDA sur Windows):
```toml
# Dans Cargo.toml
[patch.crates-io]
bindgen_cuda = { git = "https://github.com/guoqingbao/bindgen_cuda.git" }
```

### 2. Velvet Optimizer

Optimiseur custom basÃ© sur AdamW avec:
- **Entropy-adaptive LR**: Ajuste le learning rate selon l'entropie de la loss
- **Perplexity-guided momentum**: Momentum adaptatif selon la perplexitÃ©
- **Sparse-aware updates**: Optimisations pour FlyLoRA

```rust
// vesper-optimizer/src/velvet.rs
pub struct VelvetOptimizer {
    params: Vec<Tensor>,
    m: HashMap<String, Tensor>,      // First moment
    v: HashMap<String, Tensor>,      // Second moment
    config: VelvetConfig,
    step: usize,
}
```

### 3. FlyLoRA (Sparse Low-Rank Adaptation)

RÃ©duction de 75% des paramÃ¨tres via:
- DÃ©composition low-rank (A Ã— B au lieu de W)
- Masque de sparsitÃ© appris
- Rank adaptatif par layer

```rust
// vesper-core/src/flylora.rs
pub struct FlyLoRALayer {
    base_weight: Tensor,      // Poids gelÃ©s
    lora_a: Tensor,           // Down projection (d Ã— r)
    lora_b: Tensor,           // Up projection (r Ã— d)
    sparsity_mask: Tensor,    // Masque binaire
    rank: usize,              // Rank LoRA (8-64)
}
```

### 4. ERA Activation (Entropy-Regularized Activation)

Alternative Ã  GELU/SiLU avec rÃ©gularisation entropique:

```rust
// vesper-core/src/era.rs
pub fn era_activation(x: &Tensor, temperature: f32) -> Result<Tensor> {
    // ERA = x * sigmoid(x/T) * (1 + entropy_term)
    let scaled = (x / temperature as f64)?;
    let gate = candle_nn::ops::sigmoid(&scaled)?;
    let base = (x * &gate)?;
    
    // Terme entropique pour rÃ©gularisation
    let entropy = compute_entropy(&gate)?;
    let regulated = (&base * (1.0 + entropy.to_scalar::<f32>()? * 0.1) as f64)?;
    
    Ok(regulated)
}
```

### 5. Tauri Desktop App

Stack frontend moderne:
- **Tauri 2.0**: SÃ©curitÃ©, petite taille (~10MB), natif
- **React 18**: UI dÃ©clarative
- **TailwindCSS**: Styling rapide
- **IPC**: Communication Rust <-> JS via `invoke()`

```typescript
// Frontend -> Backend
const result = await invoke<BenchmarkResult>('start_benchmark', {
  config: { epochs: 10, model_size: 'Medium' }
});

// Backend events -> Frontend
await listen('benchmark-progress', (event) => {
  console.log(event.payload);
});
```

---

## âš™ï¸ Configuration

### Variables d'environnement

```powershell
# Obligatoires pour CUDA
$env:CUDA_PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8"
$env:CUDA_HOME = $env:CUDA_PATH

# Optionnel: architecture GPU spÃ©cifique
$env:CUDA_ARCH = "sm_89"  # RTX 4090/4080
# $env:CUDA_ARCH = "sm_86"  # RTX 3090/3080
# $env:CUDA_ARCH = "sm_75"  # RTX 2080/2070

# Debug CUDA
$env:CUDA_LAUNCH_BLOCKING = "1"  # Debug synchrone
```

### Architectures GPU supportÃ©es

| GPU | Architecture | Code |
|-----|--------------|------|
| RTX 4090/4080/4070 | Ada Lovelace | sm_89 |
| RTX 3090/3080/3070 | Ampere | sm_86 |
| RTX 2080/2070/2060 | Turing | sm_75 |
| GTX 1080/1070 | Pascal | sm_61 |

### Cargo.toml workspace

```toml
[workspace]
resolver = "2"
members = [
    "crates/vesper-core",
    "crates/vesper-optimizer",
    "crates/vesper-metacog",
    "crates/vesper-training",
    "crates/vesper-app",
]

[workspace.dependencies]
# Candle avec CUDA
candle-core = { git = "https://github.com/EricLBuehler/candle.git", rev = "175926c9", features = ["cuda"] }
candle-nn = { git = "https://github.com/EricLBuehler/candle.git", rev = "175926c9", features = ["cuda"] }

# CUDA bindings
cudarc = "0.10"

# Async
tokio = { version = "1", features = ["full"] }

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Tauri
tauri = { version = "2.0", features = [] }

[profile.release]
opt-level = 3
lto = "fat"        # Link-time optimization
codegen-units = 1  # Meilleure optimisation
strip = true       # RÃ©duire taille binaire

[patch.crates-io]
# Fix pour bindgen CUDA
bindgen_cuda = { git = "https://github.com/guoqingbao/bindgen_cuda.git" }
```

---

## ğŸš€ Utilisation

### Lancer l'application

```powershell
cd crates/vesper-app
npm run tauri dev
```

### Benchmark AdamW vs Velvet

Le benchmark utilise un **vrai training avec autograd** via `candle-nn`:
- **VarMap** + **VarBuilder** pour les paramÃ¨tres avec gradient tracking
- **AdamW optimizer** de candle-nn avec `backward_step()`
- **Cross-entropy loss** rÃ©elle sur les tokens
- **Perplexity** = exp(loss) affichÃ©e en temps rÃ©el

**DiffÃ©rences Velvet vs AdamW**:
| ParamÃ¨tre | AdamW | Velvet |
|-----------|-------|--------|
| Learning Rate | 1x | 1.5x (adaptatif) |
| Beta1 (momentum) | 0.9 | 0.95 |
| Weight Decay | 0.01 | 0.01 |

**Utilisation**:
1. Charger un dataset (JSON/JSONL format SQuAD supportÃ©)
2. SÃ©lectionner le nombre d'epochs
3. Cliquer "AdamW vs Velvet"
4. Observer les logs en temps rÃ©el avec loss et perplexity

### Formats de dataset supportÃ©s

```json
// Format SQuAD (recommandÃ© pour le franÃ§ais)
{
  "data": [
    {
      "paragraphs": [
        {
          "context": "Le texte du contexte...",
          "qas": [
            {
              "question": "Quelle est la question?",
              "answers": [{"text": "La rÃ©ponse"}]
            }
          ]
        }
      ]
    }
  ]
}

// Format JSONL simple
{"text": "Premier exemple de texte"}
{"text": "DeuxiÃ¨me exemple"}
```

---

## ğŸ”§ Troubleshooting

### Erreur: "nvcc not found"

```powershell
# VÃ©rifier CUDA_PATH
echo $env:CUDA_PATH
# Doit afficher: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8

# Ajouter au PATH si manquant
$env:PATH += ";$env:CUDA_PATH\bin"
```

### Erreur: "cl.exe not found" / MSVC Linker

```powershell
# Ouvrir "x64 Native Tools Command Prompt for VS 2022"
# Ou charger l'environnement manuellement:
& "C:\Program Files\Microsoft Visual Studio\2022\BuildTools\VC\Auxiliary\Build\vcvars64.bat"

# VÃ©rifier que MSVC est installÃ©:
# Visual Studio Installer > Modify > "Desktop development with C++"
# Composants requis:
#   - MSVC v143 - VS 2022 C++ x64/x86 build tools
#   - Windows 11 SDK (10.0.22621.0)
#   - C++ CMake tools for Windows
```

### Erreur: "LINK : fatal error LNK1181: cannot open input file 'cuda.lib'"

```powershell
# Le fork guoqingbao/bindgen_cuda rÃ©sout ce problÃ¨me
# VÃ©rifier dans Cargo.toml:
[patch.crates-io]
bindgen_cuda = { git = "https://github.com/guoqingbao/bindgen_cuda.git" }
```

### Erreur: "CUDA out of memory"

```powershell
# RÃ©duire batch_size dans l'UI (max 8 recommandÃ©)
# RÃ©duire seq_length (64 pour benchmark)
# Utiliser un modÃ¨le plus petit (Small au lieu de Large)
```

### Erreur: "NaN loss during training"

Les corrections ont Ã©tÃ© appliquÃ©es dans le code:
- Epsilon ajoutÃ© dans les calculs logarithmiques
- Clamping des valeurs pour Ã©viter overflow
- Learning rate rÃ©duit (max 0.0001)
- `.contiguous()` aprÃ¨s les opÃ©rations transpose

### Erreur: "Tensor 'embedding' non trouvÃ©" (Chat)

Le modÃ¨le VesperLM utilise des noms de tenseurs diffÃ©rents:
- `embeddings.weight` (pas `embedding`)
- `lm_head.weight` (pas `output_proj`)

Cette correction a Ã©tÃ© appliquÃ©e dans `commands.rs`.

### Erreur: "tokio runtime panic"

```rust
// Ne pas utiliser reqwest::blocking dans un contexte async
// Utiliser reqwest async ou std::thread::spawn
```

### Build lent

```powershell
# Utiliser sccache pour cache de compilation
cargo install sccache
$env:RUSTC_WRAPPER = "sccache"

# Ou build incrÃ©mental
cargo build  # Premier build lent
cargo build  # Builds suivants rapides
```

### Tokenizer CamemBERT non trouvÃ©

```powershell
# TÃ©lÃ©charger le tokenizer CamemBERT
huggingface-cli download camembert-base tokenizer.json

# Ou copier manuellement dans:
# C:\Users\<user>\AppData\Local\VesperAI\tokenizers\tokenizer.json
```

---

## ğŸ“Š Benchmarks & RÃ©sultats

### Configuration de test
- **GPU**: NVIDIA RTX 4080 (87% utilisation GPU atteinte)
- **CPU**: Intel i9-13900K
- **RAM**: 64GB DDR5
- **Dataset**: TinyStories (~37k tokens)

### ModÃ¨le VesperLM

| Taille | Layers | Heads | Hidden | Params |
|--------|--------|-------|--------|--------|
| Small | 6 | 6 | 384 | ~25M |
| **Medium** | 12 | 12 | 768 | **~89M** |
| Large | 24 | 16 | 1024 | ~350M |

### RÃ©sultats Training (120 epochs, Medium)

```
Epoch   1: loss=11.29 | ppl=79715
Epoch  30: loss=4.27  | ppl=71
Epoch  60: loss=2.37  | ppl=10.67
Epoch  90: loss=1.62  | ppl=5.04
Epoch 120: loss=1.22  | ppl=3.38  âœ…
```

- **Temps total**: ~2.5 minutes
- **ModÃ¨le sauvegardÃ©**: 656 MB (SafeTensors)
- **GPU utilisation**: 87% (optimal)

### Comparaison Velvet vs AdamW

**Benchmark rÃ©el (15 epochs, VesperLM Medium 89M params):**

| MÃ©trique | AdamW | Velvet | AmÃ©lioration |
|----------|-------|--------|-------------|
| Final Loss | 6.38 | **5.39** | **-15.6%** |
| Final Perplexity | 591 | **219** | **-63%** |
| Time | 18.5s | 18.9s | Similaire |
| Memory | 2000 MB | 2000 MB | Identique |

**Benchmark Ã©tendu (20 epochs):**

| MÃ©trique | AdamW | Velvet | AmÃ©lioration |
|----------|-------|--------|-------------|
| Final Loss | 5.45 | **4.48** | **-17.7%** |
| Final Perplexity | 232 | **89** | **-62%** |

**ClÃ©s du succÃ¨s Velvet:**
- âœ… Kernels CUDA custom (zero-copy, in-place updates)
- âœ… Learning Rate adaptatif (1.5x avec entropy-guided)
- âœ… Momentum adaptatif (beta1=0.95, perplexity-guided)
- âœ… Sparse-aware updates (optimisÃ© pour FlyLoRA)

### Note sur l'Overfitting

Avec un petit dataset (37k tokens), le modÃ¨le atteint une perplexitÃ© trÃ¨s basse (3.38) mais **overfit**. Pour de meilleurs rÃ©sultats de gÃ©nÃ©ralisation:
- Utiliser le dataset **Claire-Dialogue-French** (150M mots)
- Ou d'autres corpus franÃ§ais volumineux

---

## ğŸ“„ License

Proprietary - The Vesper House. All rights reserved. See [LICENSE](LICENSE).

---

## ï¿½ Datasets RecommandÃ©s

### Pour le franÃ§ais (avec CamemBERT tokenizer)

| Dataset | Taille | AccÃ¨s | Usage |
|---------|--------|-------|-------|
| **Claire-Dialogue-French** | 150M mots | Gated (HuggingFace auth) | Dialogues conversationnels |
| SQuAD-FR | ~100k Q&A | Public | Question-RÃ©ponse |
| French Wikipedia | ~2B mots | Public | Texte gÃ©nÃ©ral |

### TÃ©lÃ©charger Claire-Dialogue-French

```python
# 1. Accepter les conditions sur HuggingFace:
# https://huggingface.co/datasets/OpenLLM-France/Claire-Dialogue-French-0.1

# 2. Se connecter et tÃ©lÃ©charger:
from datasets import load_dataset
from huggingface_hub import login

login(token="hf_XXXXX")  # Token depuis huggingface.co/settings/tokens
ds = load_dataset("OpenLLM-France/Claire-Dialogue-French-0.1")

# 3. Exporter en TXT
with open("claire_train.txt", "w", encoding="utf-8") as f:
    for example in ds["train"]:
        f.write(example["text"] + "\n")
```

---

## ï¿½ğŸ™ CrÃ©dits

- **Hugging Face Candle** - Framework ML Rust
- **EricLBuehler** - Fork Candle avec fixes CUDA 12.8
- **Tauri** - Framework desktop app
- **guoqingbao** - Fix bindgen_cuda pour Windows
- **OpenLLM-France** - Dataset Claire-Dialogue-French

---

## ğŸ“ Changelog

### v0.2.0 (Janvier 2026)
- âœ… VesperLM complet avec attention, FlyLoRA, ERA
- âœ… Training CUDA fonctionnel avec autograd
- âœ… CamemBERT tokenizer intÃ©grÃ©
- âœ… Chat inference avec top-p/top-k sampling
- âœ… Memory-mapped dataset cache
- âœ… Console logs sans limite + auto-scroll
- ğŸ”§ Fix MSVC linker / bindgen_cuda
- ğŸ”§ Fix NaN loss (stabilitÃ© numÃ©rique)
- ğŸ”§ Fix tensor shapes (FlyLoRA, ERA)

### v0.1.0 (DÃ©cembre 2025)
- Initial release
- Architecture de base

---

**Built with ğŸ¦€ Rust | Powered by Candle | Accelerated by CUDA**

Made by [The Vesper House](https://github.com/thevesperhouse-hub)
